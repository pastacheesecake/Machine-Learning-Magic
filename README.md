# Machine-Learning-Magic
NLTK - Tensorflow (soon)

Writing poetry and narrative texts using statistical-demon-summoning (cause human inspiration is overrated I guess)

# Machine Learning Narrative - Poetry

## Requirements

* __Python 3__ (I suggest Anaconda 3)
* NumPy, SciPy, Stats and the like (come with Anaconda)
* __NLTK__, you should download all the corpus and associated modules [NLTK](https://www.nltk.org) (either `pip install nltk` or `conda install nltk` )
* TensorFlow for Python [TensorFlow Python3 API](https://www.tensorflow.org/api_docs/python/)
* Pickle and JSON if wanna get fancy [̲̅$̲̅(̲̅ ͡° ͜ʖ ͡°̲̅)̲̅$̲̅]	

## Poetry

It all depends on what you want to write, my personal method is quite simple:

1. Get a plain-text library of books you wanna cut-up from and save them in your project folder. *Can get PDFs but then you'd need to extract the text and PDF text-extraction in Python 3 is... not the best tbh)*
2. Tokenize the texts using `nltk.tokenize()` and dump those results somewhere *(cause we don't wanna repeat the process every time we write something)* i.e

```python
from nltk import tokenize
folder_path = 'poetry' #Directory where our data is
container = [] # Contains tokenized books to be pickled into file, probably list ain't a good idea though
for filename in glob.glob(os.path.join(folder_path, '*')):  #Finds all files in  folder_path
  with open(filename, 'r',encoding='utf-8') as f: # utf-8 magic
    text = ' '.join(f.read().splitlines()) #strips newlines
    lines_list = tokenize.sent_tokenize(text) #nltk tokenization
    container.append(lines_list)
    container = [item for sublist in container for item in sublist] #flattens the list of lists, maybe slow... fix it yourself
with open('results_poetry.txt','wb') as results:
    pickle.dump(container,results)
    results.close()
    
    
```

3. Having all your words tokenized, do cut-up magic a-la *Burroughs* or *Dada*, here's my method:
* Randomly select words from your pickle dump/text file using python's random module or maybe select specific words and whatnot.
* *Optional: If you wannna make actual sentences, use the tags generated by `nltk.tokenize()` so pick in a sensible manner i.e [Chomsky Hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy) (this will come in handy when writing stories)*
* Filter by number of syllables, use either [Pyphen](http://pyphen.org), `nltk.cmudict` or code your own syllable-counting function like a big boy ( ಠ ͜ʖ ಠ)	*pyphen is the best imo*
* Apply sentiment analysis (either pre-trained `nltk.sentiment.vader` [Docu](https://www.nltk.org/_modules/nltk/sentiment/vader.html) or train your own TensorFlow neural network like a good hee-ho *more on this later I guess*), yet again filter or do as you please

```python
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
def analyse(sentences): #Accepts list of sentences and performs sentiment intensity analysis
    for i in sentences:
        #print(i)
        ss = sid.polarity_scores(i)
        negativity.append(ss['neg'])
        positivity.append(ss['pos'])
        neutral.append(ss['neu'])
        for k in sorted(ss):
            print('{0}: {1},'.format(k, ss[k]), end='\n')
            
```
* *Profit?*


## Narrative

To be fair, there's no straight-up method to write narrative texts using NLTK or plain Python, there's several TensorFlow implementations though, but they're all equally as convoluted and nonsensical as Infinite Jest. 

1. Train your own neural network using books/texts you want to replicate (needs time, a good machine and several hours or Google Cloud if you're loaded)
2. Using a pre-trained neural network (won't help you with this one bb)
3. Cheating using NLTK tokens and Chomsky Hierarchy

### TensorFlow implementation (thanks [Sung Kim](https://github.com/hunkim))

1. Install Python3 TensorFlow API `pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl` or `conda install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl`
2. Download [Multi-layer Recurrent Neural Networks package](https://github.com/hunkim/word-rnn-tensorflow)
3. Replace the `input.txt` file in the TinyShakespeare folder of MRNNP with your training text
4. Run `train.py` (this will take several hours/computing power, check CPU/GPU temperature and whatnot)
5. Parse in sample.py [For more details](https://github.com/hunkim/word-rnn-tensorflow#beam-search)

### NLTK-Chomsky cheating
 
soon


